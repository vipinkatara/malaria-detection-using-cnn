# -*- coding: utf-8 -*-
"""malaria_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dArKnWJceOEa3Zs6_maV_T2L4O1-1kmT

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import cv2
import keras
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten, BatchNormalization

"""# Loading the data"""

DATA_DIR = './cell_images'
CATEGORIES = ['Parasitized', 'Uninfected']
data = []
IMG_SIZE = 50
for category in CATEGORIES:
    path = os.path.join(DATA_DIR, category)
    class_num = CATEGORIES.index(category)
    for img in os.listdir(path):
        try:
            img_arr = cv2.imread(os.path.join(path,img))
            new_arr = cv2.resize(img_arr, (IMG_SIZE,IMG_SIZE))
            data.append([new_arr, class_num])
        except Exception as E:
            pass

np.shape(data)

"""# Separating features and labels"""

X = []
Y = []
for features, labels in data:
    X.append(features)
    Y.append(labels)
X = np.array(X)
X = X / 255

X.shape

"""# CountPlot"""

sns.countplot(Y)
plt.show()

"""# Plotting some images"""

inde = np.random.randint(0 , 6543 , 12)
r = 0
for index in inde:
    r += 1
    plt.subplot(3,4,r)
    #grid  = X[index].reshape(IMG_SIZE,IMG_SIZE)
    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)
    plt.imshow(X[index])
    plt.xticks([]) , plt.yticks([])
    plt.title(CATEGORIES[Y[index]])
plt.show()

np.unique(Y, return_counts=True)

#one hot encoding of labels
Y = to_categorical(Y)

"""# Spliting data into train, test, validation"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)

print('Training Images : ',end='')
print(x_train.shape[0])

print('Testing Images : ',end='')
print(x_test.shape[0])

print('Validation Images : ',end='')

print(x_val.shape[0])

"""# CNN"""

model = Sequential()
model.add(Conv2D(64,3,data_format='channels_last',padding='same',activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))
model.add(MaxPool2D(pool_size=(3,3)))
model.add(Conv2D(32, 3, data_format='channels_last',activation='relu', padding='same'))

model.add(MaxPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(128))
model.add(Dropout(0.5))
model.add(Dense(64))
model.add(Dropout(0.5))
model.add(Dense(2))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""# Data augumentation"""

from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images

datagen.fit(x_train)

"""# Images after augumentation"""

for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):
	# create a grid of 3x3 images
	for i in range(0, 9):
		plt.subplot(330 + 1 + i)
		plt.imshow(X_batch[i])
	# show the plot
	plt.show()
	break

"""# Model fitting"""

history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=32), epochs=100, validation_data= (x_val,y_val))

"""# Predicting"""

y_pred = model.predict(x_test)

"""# accuracy metrics"""

from sklearn.metrics import accuracy_score
accuracy_score(y_pred.round(), y_test)

test_pred = np.argmax(y_pred, axis=1)
y_test_pred = np.argmax(y_test, axis=1)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_pred,y_test_pred)
sns.set(font_scale=1.4) # for label size
sns.heatmap(cm, annot=True, annot_kws={"size": 16}) # font size

plt.show()
print(cm)



"""# training vs validation loss"""

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""training vs validation accuracy"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()